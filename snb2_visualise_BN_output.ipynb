{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc79abd",
   "metadata": {},
   "source": [
    "# File Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7433f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import pysmile\n",
    "import pysmile_license\n",
    "import sys\n",
    "import json\n",
    "from colormap import rgb2hex\n",
    "from collections import ChainMap\n",
    "import re\n",
    "from scipy.stats import lognorm\n",
    "import scipy\n",
    "import math\n",
    "from scipy import stats\n",
    "from IPython import embed\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "import shapely\n",
    "from shapely.geometry import LineString, shape\n",
    "from folium.plugins import FloatImage\n",
    "import itertools\n",
    "import pyproj\n",
    "import ipywidgets\n",
    "\n",
    "# Transforming from m to degrees\n",
    "transformer = \\\n",
    "    pyproj.Transformer.from_crs(pyproj.CRS(\"EPSG:32760\"),pyproj.CRS(\"EPSG:4326\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519eab6",
   "metadata": {},
   "source": [
    "# Load and format data, network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ff07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Processed_data_and_output/Autogen_clusters/number_of_clusts.txt') as f:\n",
    "    cluster_count = int(f.readlines()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c8ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is duplicated from BN notebook. Need to remove this\n",
    "def data_gen(clust,shoreline_nodes,binary_nodes,node_exceptions,nodes_to_consider,df_shoreline,net):\n",
    "    df_shoreline = df_shoreline[df_shoreline.proxy=='TOB']\n",
    "    \n",
    "    df_shoreline.index = df_shoreline.year2\n",
    "\n",
    "    df_shoreline = df_shoreline.rename(\n",
    "        columns={\n",
    "            'intersect_distance':'TOB',\n",
    "            'previous_mean_shoreline_change':'Previous',\n",
    "            'other_locs':'Surroundings',\n",
    "            'shoreline_direction':'ShorelineDirection',\n",
    "            'avg_slope_change':'ShorelineCurvature',\n",
    "            'facing_north?':'Facingnorth',\n",
    "            'facing_east?':'Facingeast',\n",
    "            'wave_energy_mid':'WavesPerpendicular',\n",
    "            'wave_energy_right':'WavesRight',\n",
    "            'wave_energy_left':'WavesLeft',\n",
    "        })\n",
    "    \n",
    "    [df_shoreline.rename(columns={x:x.capitalize()},inplace=True) for x in list(df_shoreline) if x not in ['lat','lon','lat_avg','lon_avg','x','y','lat_new','lon_new']]\n",
    "    \n",
    "    #normalise the shoreline change variables\n",
    "    for var in shoreline_nodes:\n",
    "        val_list = []\n",
    "        for index,row in df_shoreline.iterrows():\n",
    "            if row[var]<-3:\n",
    "                val = 'Erosion'\n",
    "            elif row[var]>3:\n",
    "                val = 'Accretion'\n",
    "            else:\n",
    "                val = 'Stable'\n",
    "            val_list.append(val)\n",
    "        \n",
    "        df_shoreline[var] = val_list\n",
    "\n",
    "        \n",
    "    df_shoreline = df_shoreline[df_shoreline.Mei<=0.5] # removing el nino since there's not enough data\n",
    "    df_shoreline['Mei'] = ['Neutral' if (x>=-0.5) else 'LaNina' for x in df_shoreline.Mei]\n",
    "    \n",
    "    net_nodes = net.get_all_node_ids()\n",
    "    net_nodes = list(set([re.sub(r'\\d+','',x) for x in net_nodes]))\n",
    "    \n",
    "    for var in list(df_shoreline[net_nodes]):\n",
    "        if var not in shoreline_nodes+node_exceptions+binary_nodes:\n",
    "            df_shoreline[var] = ['Low' if (x<np.mean(df_shoreline[var])) else 'High' for x in df_shoreline[var]]\n",
    "            \n",
    "    for var in list(df_shoreline[net_nodes]):\n",
    "        if type(df_shoreline.reset_index().loc[0,var])==np.bool_:\n",
    "            df_shoreline[var] = df_shoreline[var].astype(str)\n",
    "\n",
    "    return(df_shoreline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "448078d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoreline_list = []\n",
    "\n",
    "for clust in range(cluster_count):\n",
    "    df_shoreline = pd.read_csv(f'Processed_data_and_output/Autogen_clusters/clust_{clust}.csv')\n",
    "\n",
    "    df_shoreline['lat_avg'] = [transformer.transform(x,y)[0] for x,y in zip(df_shoreline['x_avg'],df_shoreline['y_avg'])]\n",
    "    df_shoreline['lon_avg'] = [transformer.transform(x,y)[1] for x,y in zip(df_shoreline['x_avg'],df_shoreline['y_avg'])]\n",
    "\n",
    "    df_shoreline['lat_new'] = [transformer.transform(x,y)[0] for x,y in zip(df_shoreline['x_new'],df_shoreline['y_new'])]\n",
    "    df_shoreline['lon_new'] = [transformer.transform(x,y)[1] for x,y in zip(df_shoreline['x_new'],df_shoreline['y_new'])]\n",
    "\n",
    "    # Pick a random year to work with\n",
    "    df_shoreline = df_shoreline[df_shoreline.year2==2005]\n",
    "    \n",
    "    shoreline_list+=[df_shoreline]\n",
    "\n",
    "df_shoreline_combined = pd.concat(shoreline_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1526541",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_864399/481322906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Processed_data_and_output/reversed_arcs_combined.xdsl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdf_shoreline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclust\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshoreline_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnode_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnodes_to_consider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_shoreline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     all_clusters_dict.update({\n",
      "\u001b[0;32m/tmp/ipykernel_864399/1649426803.py\u001b[0m in \u001b[0;36mdata_gen\u001b[0;34m(clust, shoreline_nodes, binary_nodes, node_exceptions, nodes_to_consider, df_shoreline, net)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_shoreline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet_nodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_shoreline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mdf_shoreline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_shoreline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    887\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0;31m# no multi-index, so validate all of the indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0;31m# We don't need to check for tuples here because those are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m#  caught by the _is_nested_tuple_indexer check above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                 \u001b[0;31m# We should never have a scalar section here, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# fall thru to straight lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;31m# GH#5667 this will fail if the label is not present in the axis.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_handle_lowerdim_multi_index_axis0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   3737\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected label or tuple of labels, got {key}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3739\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3741\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    351\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "all_clusters_dict = {}\n",
    "net_dict = {}\n",
    "\n",
    "for clust in range(cluster_count):\n",
    "    df_shoreline = pd.read_csv(f'Processed_data_and_output/Autogen_clusters/clust_{clust}.csv')\n",
    "\n",
    "    df_shoreline['lat_avg'] = [transformer.transform(x,y)[0] for x,y in zip(df_shoreline['x_avg'],df_shoreline['y_avg'])]\n",
    "    df_shoreline['lon_avg'] = [transformer.transform(x,y)[1] for x,y in zip(df_shoreline['x_avg'],df_shoreline['y_avg'])]\n",
    "\n",
    "    df_shoreline['lat_new'] = [transformer.transform(x,y)[0] for x,y in zip(df_shoreline['x_new'],df_shoreline['y_new'])]\n",
    "    df_shoreline['lon_new'] = [transformer.transform(x,y)[1] for x,y in zip(df_shoreline['x_new'],df_shoreline['y_new'])]\n",
    "\n",
    "    # Pick a random year to work with\n",
    "    df_shoreline = df_shoreline[df_shoreline.year2==2005]\n",
    "\n",
    "    shoreline_nodes = ['Tob','Previous']# 'Surroundings','Previous'\n",
    "    binary_nodes = ['Tcseason']\n",
    "    node_exceptions = ['Mei','Season']\n",
    "    nodes_to_consider = ['TOB','MEI','TCSeason','ShorelineCurvature','season','Facingnorth','Facingeast','Previous',\n",
    "                    'WavesPerpendicular','WavesRight','WavesLeft']     \n",
    "    net = pysmile.Network()\n",
    "    net.read_file(f'Processed_data_and_output/reversed_arcs_combined.xdsl')\n",
    "\n",
    "    df_shoreline = data_gen(clust,shoreline_nodes,binary_nodes,node_exceptions,nodes_to_consider,df_shoreline,net)\n",
    "    \n",
    "    all_clusters_dict.update({\n",
    "        clust:df_shoreline\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9b0de8",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = pysmile.Network()\n",
    "net.read_file(f'Processed_data_and_output/reversed_arcs_combined.xdsl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d23c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(f'Processed_data_and_output/reversed_arcs_combined.xdsl')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in root:\n",
    "    print(child.tag,child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f14df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_dict = {}\n",
    "\n",
    "for case in root[1]:\n",
    "    if case.attrib['name']=='testcase':\n",
    "        for node_case in case:\n",
    "            evidence_dict.update({\n",
    "                node_case.attrib['node']:node_case.attrib['state']\n",
    "            })\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_node = 'Tob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb942946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shoreline_map(net,target_node):\n",
    "    \n",
    "    view = 'Satellite'\n",
    "    location = [-5.667723, 176.094928]\n",
    "    if view == 'Map':\n",
    "        map_osm = folium.Map(location=location,zoom_start=11)\n",
    "    elif view == 'Satellite':\n",
    "        token = \"pk.eyJ1Ijoic2hhbm5vbi1iZW5ndHNvbiIsImEiOiJja3F1Y2Q0dHEwMzYwMm9wYmtzYzk2bDZuIn0.5jGMyEiJdmXs1HL7x3ThPw\" # your mapbox token\n",
    "        tileurl = 'https://api.mapbox.com/v4/mapbox.satellite/{z}/{x}/{y}@2x.png?access_token=' + str(token)\n",
    "\n",
    "        map_osm = folium.Map(location=location, zoom_start=40, tiles=tileurl, attr='Mapbox')\n",
    "\n",
    "    output_list = []\n",
    "    \n",
    "    net.clear_all_evidence()\n",
    "\n",
    "    for clust in range(cluster_count):\n",
    "        \n",
    "        node_id = target_node+str(clust)\n",
    "        \n",
    "        # Update network evidence and get beliefs\n",
    "        \n",
    "        \n",
    "        evidence_dict_subset = {key:value for key,value in evidence_dict.items() if re.sub('\\D', '',key)==str(clust)}\n",
    "\n",
    "        for node,value in evidence_dict_subset.items():\n",
    "\n",
    "            children = net.get_child_ids(node)\n",
    "            node_id = net.get_node(node)\n",
    "\n",
    "            for child in children:\n",
    "                child_outcomes = net.get_outcome_ids(child)\n",
    "                child_evidence_dict = {}\n",
    "                for child_outcome in child_outcomes:\n",
    "\n",
    "                    child_evidence_dict.update({\n",
    "                        child_outcome:[1 if row[child]==child_outcome else 0][0]\n",
    "                    })\n",
    "\n",
    "                evidence_list = [child_evidence_dict[x] for x in child_outcomes]\n",
    "                net.set_virtual_evidence(child,evidence_list)\n",
    "\n",
    "    net.update_beliefs()\n",
    "    \n",
    "    for clust,group in df_shoreline_combined.groupby(f'clust_{clust}'):\n",
    "        feature_gp1 = folium.FeatureGroup(name=f\"Cluster {clust}\")\n",
    "        for index,row in group.iterrows():\n",
    "            folium.CircleMarker(location=[row.lat_avg,row.lon_avg],radius=1,weight=5).add_to(feature_gp1)\n",
    "                                    # popup=pd.DataFrame(row[['lat','lon','Tcseason','Mei','Spring','Winter','Autum','Summer','Tob']])).add_to(feature_gp1)\n",
    "        feature_gp1.add_to(map_osm)    \n",
    "       \n",
    "    \n",
    "    for index,row in df_shoreline_combined.iterrows():\n",
    "        \n",
    "        \n",
    "         \n",
    "        clust = row[f'clust_{cluster_count}']\n",
    "        node_id = target_node+str(clust)\n",
    "        \n",
    "        probs_model = {outcome:val for outcome,val in zip(net.get_outcome_ids(node_id),net.get_node_value(node_id))}\n",
    "\n",
    "        most_prob_state = max(probs_model,key=probs_model.get)\n",
    "\n",
    "        opacity = (probs_model[most_prob_state]-0.34)/(1-0.34)\n",
    "\n",
    "        if most_prob_state=='Erosion':\n",
    "                folium.PolyLine(locations=[[row.lat_avg,row.lon_avg],\n",
    "                               [row.lat_avg-(row.lat_new-row.lat_avg)/10,row.lon_avg-(row.lon_new-row.lon_avg)/10]],\n",
    "                                color='red',opacity=opacity).add_to(map_osm)\n",
    "        elif most_prob_state=='Accretion':\n",
    "                folium.PolyLine(locations=[[row.lat_avg,row.lon_avg],\n",
    "                               [row.lat_avg+(row.lat_new-row.lat_avg)/10,row.lon_avg+(row.lon_new-row.lon_avg)/10]],\n",
    "                                color='blue',opacity=opacity).add_to(map_osm)\n",
    "        elif most_prob_state=='Stable':\n",
    "                folium.PolyLine(locations=[[row.lat_avg-(row.lat_new-row.lat_avg)/40,row.lon_avg-(row.lon_new-row.lon_avg)/40],\n",
    "                               [row.lat_avg+(row.lat_new-row.lat_avg)/40,row.lon_avg+(row.lon_new-row.lon_avg)/40]],\n",
    "                                color='white',opacity=opacity).add_to(map_osm)\n",
    "                    \n",
    "    \n",
    "    folium.LayerControl().add_to(map_osm)\n",
    "\n",
    "    return(map_osm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a44893",
   "metadata": {},
   "source": [
    "# Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shoreline_map(net,target_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ipywidgets.Dropdown(\n",
    "options=['LaNina','Mean'],\n",
    "value='LaNina',\n",
    "description='ENSO',\n",
    "disabled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update network evidence and get beliefs\n",
    "net.clear_all_evidence()\n",
    "\n",
    "for node,value in evidence_dict.items():\n",
    "    \n",
    "    children = net.get_child_ids(node)\n",
    "    node_id = net.get_node(node)\n",
    "    \n",
    "    for child in children:\n",
    "        child_outcomes = net.get_outcome_ids(child)\n",
    "        child_evidence_dict = {}\n",
    "        for child_outcome in child_outcomes:\n",
    "\n",
    "            child_evidence_dict.update({\n",
    "                child_outcome:[1 if row[child]==child_outcome else 0][0]\n",
    "            })\n",
    "\n",
    "        evidence_list = [child_evidence_dict[x] for x in child_outcomes]\n",
    "        net.set_virtual_evidence(child,evidence_list)\n",
    "\n",
    "net.update_beliefs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ccf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_model = {outcome:val for outcome,val in zip(net.get_outcome_ids(node_id),net.get_node_value(node_id))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d74b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probs_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f2bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f7ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
