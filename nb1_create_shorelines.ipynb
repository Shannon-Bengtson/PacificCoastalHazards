{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abaad99",
   "metadata": {},
   "source": [
    "# File Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a10297",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* All the shoreline change estimates are with respect to the year 2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e0e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import datetime\n",
    "import itertools\n",
    "import shapely \n",
    "from shapely.geometry import LineString, shape\n",
    "from scipy import interpolate\n",
    "import pyproj\n",
    "import xarray as xr\n",
    "import os\n",
    "import pickle\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "transformer = \\\n",
    "    pyproj.Transformer.from_crs(pyproj.CRS(\"EPSG:32760\"),pyproj.CRS(\"EPSG:4326\")) \n",
    "\n",
    "transformer2 = \\\n",
    "    pyproj.Transformer.from_crs(pyproj.CRS(\"EPSG:28356\"),pyproj.CRS(\"EPSG:4326\")) \n",
    "\n",
    "\n",
    "def multipoint_to_linestring(multipoint):\n",
    "    return LineString(multipoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c749c-a370-441b-bf1f-d4ccc5fd5947",
   "metadata": {},
   "source": [
    "# Load coastSat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977533ff-1603-4ec2-b4d8-f342fe7dd9ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shannonb/miniconda3/envs/pac_model/lib/python3.7/site-packages/ipykernel_launcher.py:26: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "/home/shannonb/miniconda3/envs/pac_model/lib/python3.7/site-packages/shapely/geometry/linestring.py:46: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  ret = geos_linestring_from_py(coordinates)\n"
     ]
    }
   ],
   "source": [
    "geojson_file = \"Preprocessed_datasets/target_file.geojson\"\n",
    "gdf_nanumea = gpd.read_file(geojson_file)\n",
    "\n",
    "gdf_nanumea['layer'] = ['Nanumea_TOB_'+x.split('-')[0] for x in gdf_nanumea['date']]\n",
    "gdf_nanumea['id'] = 1\n",
    "gdf_nanumea['geometry'] = gdf_nanumea['geometry'].apply(multipoint_to_linestring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb273a7f-aa9f-4a5a-9007-1d40f35dd706",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shannonb/miniconda3/envs/pac_model/lib/python3.7/site-packages/pandas/core/dtypes/cast.py:112: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  values = construct_1d_object_array_from_listlike(values)\n"
     ]
    }
   ],
   "source": [
    "# EPSG:3832 32660\n",
    "transformer3 = \\\n",
    "    pyproj.Transformer.from_crs(pyproj.CRS(\"EPSG:2102\"),pyproj.CRS(\"EPSG:4326\")) \n",
    "\n",
    "gdf_nanumea['geometry_2'] = [LineString([transformer3.transform(y,x) for x,y in geo.coords]) for geo in gdf_nanumea.geometry]\n",
    "gdf_nanumea['geometry_3'] = [LineString([transformer3.transform(x,y) for x,y in geo.coords]) for geo in gdf_nanumea.geometry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bab2ea1f-02a3-4aa3-80b2-9dee3a23d98f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates_dict = {(int(x.split('-')[0])+(int(x.split('-')[1])-0.5)/12):datetime.datetime(int(x.split('-')[0]),int(x.split('-')[1]),int(x.split('-')[2].split('T')[0])) for x in gdf_nanumea.date}\n",
    "with open('dates_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(dates_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af4afe",
   "metadata": {},
   "source": [
    "# Load the shoreline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99eb462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using geopandas\n",
    "\n",
    "base_dir = r\"/home/shannonb/Tuvalu_shoreline_change\"\n",
    "\n",
    "proxies = [\n",
    "    # r'TOB',\n",
    "    # r'VL',\n",
    "    r'WM'\n",
    "]\n",
    "\n",
    "atolls = [\n",
    "    'Nanumea'\n",
    "]\n",
    "\n",
    "locations_dict = {\n",
    "    'Nanumea':[-5.667723, 176.094928]\n",
    "}\n",
    "\n",
    "combinations = list(itertools.product(atolls,proxies))\n",
    "\n",
    "# Define the years the shoreline change datafile is for. This is atoll specific\n",
    "years_dict = {\n",
    "    'Nanumea':'2003_2021'\n",
    "}\n",
    "\n",
    "geopandas_dict = {}\n",
    "\n",
    "# There is one shape file per proxy, per year, per atoll\n",
    "for combination in combinations:\n",
    "    atoll = combination[0]\n",
    "    proxy = combination[1]\n",
    "    # year = years_dict[combination[0]]\n",
    "    #test = gpd.read_file(base_dir+'/Preprocessed_datasets/Shoreline_shapefiles/{}/{}_{}_{}.shp'.format(proxy,atoll,proxy,year)).rename(columns={'Layer':'layer','Area':'area','Perimeter':'perimeter'})\n",
    "    \n",
    "    geopandas_dict.update({\n",
    "        (atoll,proxy):gdf_nanumea\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ba3ce",
   "metadata": {},
   "source": [
    "# Format Shoreline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c7e029b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# converting the geopandas dataframes into a pandas dataframe\n",
    "\n",
    "df_xy_dict = {}\n",
    "\n",
    "for combination in combinations:\n",
    "    atoll = combination[0]\n",
    "    proxy = combination[1]\n",
    "    year = years_dict[combination[0]]\n",
    "    \n",
    "    gdf_test = geopandas_dict[atoll,proxy].copy()\n",
    "    \n",
    "    # # There are some typos in the raw data which need to be corrected/accounted for\n",
    "    # gdf_test.loc[gdf_test.layer=='Nanumea_WM_2015_','layer'] = 'Nanumea_WM_2015'\n",
    "\n",
    "    gdf_test['layer'] = gdf_test['layer'].fillna(value=1)\n",
    "    gdf_test['id'] = gdf_test['id'].fillna(value=1)\n",
    "    gdf_test['id'] = gdf_test.id.astype(int)\n",
    "    \n",
    "    gdf_test = gdf_test[~gdf_test[['layer','geometry']].isna().any(axis=1)]\n",
    "\n",
    "    years_list = []\n",
    "    gdf_right_years_dict = {}\n",
    "\n",
    "    for layer,group in gdf_test.groupby('layer'):\n",
    "        if (proxy=='VL')&(atoll!='Nanumaga'):\n",
    "            year = int(layer.split('_')[1])\n",
    "        else:\n",
    "            year = int(layer.split('_')[-1])\n",
    "        years_list.append(year)\n",
    "\n",
    "        gdf_right_years_dict.update({\n",
    "            year:group\n",
    "        })\n",
    "\n",
    "    gdf_test = pd.concat(gdf_right_years_dict)\n",
    "    gdf_test.reset_index(drop=False,inplace=True)\n",
    "    \n",
    "    gdf_test['level_0'] = gdf_test.level_0.astype(int)\n",
    "    gdf_test['id'] = gdf_test.id.astype(int)\n",
    "    gdf_test = gdf_test.rename(columns={'level_0':'year'}).drop('level_1',axis=1)\n",
    "    \n",
    "    # NB: ids 1 and 3 in the wrong way for one year in raw data\n",
    "    if (atoll=='Nanumea')&(proxy=='TOB'):\n",
    "        gdf_test.loc[gdf_test.year==2009,'id'] = gdf_test.loc[gdf_test.year==2009,'id'].replace(1,1000)\n",
    "        gdf_test.loc[gdf_test.year==2009,'id'] = gdf_test.loc[gdf_test.year==2009,'id'].replace(3,1)\n",
    "        gdf_test.loc[gdf_test.year==2009,'id'] = gdf_test.loc[gdf_test.year==2009,'id'].replace(1000,3)\n",
    "        \n",
    "\n",
    "    # Convert polygons to linestrings if there are any (ome inconsisent formating)\n",
    "    if type(gdf_test.loc[0,'geometry'])==shapely.geometry.Polygon:\n",
    "        gdf_test['geometry'] = [x.boundary for x in gdf_test.geometry]\n",
    "        \n",
    "    # Based on the length of the line segments, calculate the number of shoreline segments for this islet\n",
    "    shape = gdf_test.loc[0,'geometry']\n",
    "    list_of_line_segments = list(shape.coords)\n",
    "    total_seg_length = 0\n",
    "    for seg_1,seg_2 in zip(list_of_line_segments[:-1],list_of_line_segments[1:]):\n",
    "        total_seg_length+=((seg_1[0]-seg_2[0])**2+(seg_1[1]-seg_2[1])**2)**0.5\n",
    "        \n",
    "    line_segment_length_after_interp = 35\n",
    "    total_number_of_line_segments = 200#int(np.round(total_seg_length/35))\n",
    "        \n",
    "    # Interpolate the linestrings so that they are all of the same length (x)\n",
    "    x = total_number_of_line_segments #number of points to interpolate\n",
    "    for i in np.arange(0,len(gdf_test),1):\n",
    "        gdf_test.loc[i,'geometry'] = \\\n",
    "            shapely.geometry.linestring.LineString(\n",
    "                [gdf_test.loc[i,'geometry'].interpolate((j/x), normalized=True) for j in range(1, x)]\n",
    "            )\n",
    "\n",
    "    dict_of_df_xy = {}\n",
    "    # Format the coordinates, and put into a dict\n",
    "    for idx,row in gdf_test.iterrows():\n",
    "        linestring = row.geometry\n",
    "        XY_list = []\n",
    "        \n",
    "        if type(linestring)==shapely.geometry.linestring.LineString:\n",
    "            XY_list = XY_list+[(x,y) for x,y in linestring.coords]\n",
    "        elif type(linestring)==shapely.geometry.multilinestring.MultiLineString:\n",
    "            XY_list = XY_list+[(x,y) for x,y in linestring[0].coords]\n",
    "                \n",
    "        df_xy = pd.DataFrame(XY_list)\n",
    "        df_xy.columns = ['lon','lat']\n",
    "        df_xy['id'] = int(row.id)\n",
    "        df_xy['year'] = row.year+(int(row.date.split('-')[1])-0.5)/12#int(row.year) #year column now is a decimal\n",
    "        \n",
    "        dict_of_df_xy.update({\n",
    "            idx:df_xy\n",
    "        })\n",
    "\n",
    "    df_xy = pd.concat(dict_of_df_xy)\n",
    "\n",
    "    df_xy = df_xy.reset_index(drop=True)\n",
    "    df_xy['x'] = df_xy.lon\n",
    "    df_xy['y'] = df_xy.lat\n",
    "    \n",
    "    df_xy[('lon,lat')] = [transformer2.transform(x,y) for x,y in zip(df_xy.lon,df_xy.lat)]\n",
    "    df_xy['lon'] = [x[1] for x in df_xy[('lon,lat')]]\n",
    "    df_xy['lat'] = [x[0] for x in df_xy[('lon,lat')]]\n",
    "    \n",
    "    print(np.unique(df_xy.id))\n",
    "    print(np.unique(gdf_test.id))\n",
    "    \n",
    "    df_xy_dict.update({\n",
    "        (atoll,proxy):{\n",
    "            'df_xy':df_xy,\n",
    "            'gdf_test':gdf_test\n",
    "        }\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab308f5",
   "metadata": {},
   "source": [
    "# Creating Transects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7b5846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section is for creating shoreline transects, it's mostly trig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401a027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shoreline_df(df_xy):\n",
    "    \n",
    "    df_temp = df_xy[~df_xy.index.isin(list(df_xy.head(1).index))].append(df_xy[df_xy.index.isin(list(df_xy.head(1).index))])\n",
    "\n",
    "    df_shoreline = pd.DataFrame({'x':df_xy.x,\n",
    "                                 'x+n':list(df_temp.x),\n",
    "                                 'y':df_xy.y,\n",
    "                                 'y+n':list(df_temp.y),\n",
    "                                 'lon':df_xy.lon,\n",
    "                                 'lat':df_xy.lat})\n",
    "\n",
    "    # Calculate the gradient of the line between the two shoreline points\n",
    "    df_shoreline['m_shoreline'] = (df_shoreline['y']-df_shoreline['y+n'])/(df_shoreline['x']-df_shoreline['x+n'])\n",
    "    df_shoreline.dropna(axis=0,inplace=True)\n",
    "    df_shoreline = df_shoreline[(df_shoreline['x']-df_shoreline['x+n'])!=0]\n",
    "\n",
    "    # Find the inverse of the gradient (because we are wanting the line that is perpendicular to the shoreline)\n",
    "    df_shoreline['m_transect'] = -df_shoreline['m_shoreline']**-1\n",
    "\n",
    "    df_shoreline['x_avg'] = [x/2 for x in (df_shoreline['x']+df_shoreline['x+n'])]\n",
    "    df_shoreline['y_avg'] = [y/2 for y in (df_shoreline['y']+df_shoreline['y+n'])]\n",
    "\n",
    "    df_shoreline['c_shoreline'] = df_shoreline['y_avg']-df_shoreline['m_shoreline']*df_shoreline['x_avg']\n",
    "    df_shoreline['c_transect'] = df_shoreline['y_avg']-df_shoreline['m_transect']*df_shoreline['x_avg']\n",
    "\n",
    "    H = 1000 # length of the transect line\n",
    "    df_shoreline['delta_y'] = [abs(H*math.sin(math.atan(m))) for m in df_shoreline['m_transect']]\n",
    "    df_shoreline['delta_x'] = [abs(H*math.cos(math.atan(m))) for m in df_shoreline['m_transect']]\n",
    "    \n",
    "    df_shoreline = df_shoreline[df_shoreline.m_shoreline!=0]\n",
    "    \n",
    "    coords_dict = {}\n",
    "    for index,row in df_shoreline.iterrows():\n",
    "        \n",
    "        if (row.y<row['y+n'])&(row.m_shoreline>0):\n",
    "            coords_dict.update({\n",
    "                index:{\n",
    "                'x_new':row.x_avg-row.delta_x,\n",
    "                'y_new':row.y_avg+row.delta_y\n",
    "                }\n",
    "            })\n",
    "        elif (row.y>row['y+n'])&(row.m_shoreline>0):\n",
    "            coords_dict.update({\n",
    "                index:{\n",
    "                'x_new':row.x_avg+row.delta_x,\n",
    "                'y_new':row.y_avg-row.delta_y\n",
    "                }\n",
    "            })\n",
    "        elif (row.y>row['y+n'])&(row.m_shoreline<0):\n",
    "            coords_dict.update({\n",
    "                index:{\n",
    "                'x_new':row.x_avg+row.delta_x,\n",
    "                'y_new':row.y_avg+row.delta_y\n",
    "                }\n",
    "            })\n",
    "        elif (row.y<row['y+n'])&(row.m_shoreline<0):\n",
    "            coords_dict.update({\n",
    "                index:{\n",
    "                'x_new':row.x_avg-row.delta_x,\n",
    "                'y_new':row.y_avg-row.delta_y\n",
    "                }\n",
    "            })\n",
    "        else:\n",
    "            asdf\n",
    "        \n",
    "    df_coords = pd.DataFrame.from_dict(coords_dict,orient='index')\n",
    "    df_shoreline = df_shoreline.join(df_coords)\n",
    "\n",
    "    return(df_shoreline)\n",
    "\n",
    "\n",
    "def calc_shoreline_change(df_shoreline_1,df_shoreline_2):\n",
    "    '''\n",
    "        Function for finding shoreline change between two years\n",
    "    '''\n",
    "    ### Now you need to find the distance between the two shorelines using the transects\n",
    "    shoreline_2_updated_dict = {}\n",
    "\n",
    "    # loop over each transect\n",
    "    for idx,row in df_shoreline_2.iterrows():\n",
    "        \n",
    "        df_intersection = df_shoreline_1.copy()\n",
    "        \n",
    "        df_intersection['x_intersect_location'] = (df_intersection.c_shoreline-row.c_transect)/(row.m_transect-df_intersection.m_shoreline)\n",
    "        df_intersection['y_intersect_location'] = row['m_transect']*df_intersection['x_intersect_location']+row['c_transect']\n",
    "        df_intersection['intersect_distance'] = np.sqrt((df_intersection['y_intersect_location']-row['y_avg'])**2+\\\n",
    "        (df_intersection['x_intersect_location']-row['x_avg'])**2)\n",
    "\n",
    "        # Find which shoreline segments would intersect with the shoreline\n",
    "        df_intersection = df_intersection[[(x<x_int)&(x_n>x_int)|(x>x_int)&(x_n<x_int)for x,x_int,x_n in zip(df_intersection.x,df_intersection.x_intersect_location,df_intersection['x+n'])]]\n",
    "        df_intersection = df_intersection[[(y<y_int)&(y_n>y_int)|(y>y_int)&(y_n<y_int) for y,y_int,y_n in zip(df_intersection.y,df_intersection.y_intersect_location,df_intersection['y+n'])]]\n",
    "#         There may be multiple, so find the closest one (that *should* be the right one)\n",
    "        \n",
    "        try:\n",
    "            df_intersection = df_intersection[(df_intersection['intersect_distance']==np.min(df_intersection['intersect_distance']))]\n",
    "            intersect_distance = df_intersection.intersect_distance.reset_index(drop=True)[0]\n",
    "            outside = (((df_intersection.x_intersect_location<row.x_avg)&(row.x_new<df_intersection.x_intersect_location))|\\\n",
    "((df_intersection.x_intersect_location>row.x_avg)&(row.x_new>df_intersection.x_intersect_location))).reset_index(drop=True)[0]\n",
    "            if outside:\n",
    "                intersect_distance = -intersect_distance\n",
    "        except:\n",
    "            intersect_distance = 0\n",
    "            \n",
    "        row['intersect_distance'] = intersect_distance\n",
    "        \n",
    "        shoreline_2_updated_dict.update({\n",
    "            idx:row\n",
    "        })\n",
    "\n",
    "    df_shoreline_2 = pd.DataFrame.from_dict(shoreline_2_updated_dict,orient='index')\n",
    "\n",
    "    df_shoreline_2['transect_angle'] = \\\n",
    "        np.arctan((df_shoreline_2.y_new-df_shoreline_2.y_avg)/(df_shoreline_2.x_new-df_shoreline_2.x_avg))*180/np.pi\n",
    "\n",
    "    return(df_shoreline_2)\n",
    "\n",
    "\n",
    "def calc_shoreline_slope_change(df_shoreline):\n",
    "    '''\n",
    "        Calc the direction the shoreline is facing\n",
    "    '''\n",
    "    \n",
    "    df_shoreline = pd.concat([df_shoreline,df_shoreline])\n",
    "\n",
    "    df_shoreline = df_shoreline.reset_index(drop=True)\n",
    "\n",
    "    df_shoreline_dict = {}\n",
    "\n",
    "    for (idx_1,row_1),(idx_2,row_2),(idx_3,row_3) in zip(\n",
    "        df_shoreline[4:].iterrows(),\n",
    "        df_shoreline[2:-2].iterrows(),\n",
    "        df_shoreline[:-4].iterrows()):\n",
    "        \n",
    "        row_2['avg_slope_change'] = np.mean([\n",
    "            (180*np.arctan((row_1.m_shoreline-row_2.m_shoreline)/(1+row_1.m_shoreline*row_2.m_shoreline))/np.pi),\n",
    "            (180*np.arctan((row_2.m_shoreline-row_3.m_shoreline)/(1+row_2.m_shoreline*row_3.m_shoreline))/np.pi)\n",
    "        ])\n",
    "\n",
    "        df_shoreline_dict.update({\n",
    "            idx_2:row_2\n",
    "        })\n",
    "\n",
    "    # only look at points where there is a significant change\n",
    "    df_shoreline = pd.DataFrame.from_dict(df_shoreline_dict,orient='index')\n",
    "    \n",
    "    # Drop the duplicates\n",
    "    df_shoreline = df_shoreline.drop_duplicates()\n",
    "    df_shoreline.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    #### Now also add in the angle (degrees) that the shoreline is facing\n",
    "    # Quadrant 1\n",
    "    df_shoreline.loc[(df_shoreline.x_new>df_shoreline.x_avg)&(df_shoreline.y_new>df_shoreline.y_avg),'shoreline_direction']=\\\n",
    "        [180*math.atan(np.abs(x_new-x_avg)/np.abs(y_new-y_avg))/np.pi for y_new,y_avg,x_new,x_avg in zip(\n",
    "                df_shoreline.y_new,\n",
    "                df_shoreline.y_avg,\n",
    "                df_shoreline.x_new,\n",
    "                df_shoreline.x_avg\n",
    "                ) if (x_new>x_avg)&(y_new>y_avg)]\n",
    "\n",
    "    # Quadrant 2\n",
    "    df_shoreline.loc[(df_shoreline.x_new>df_shoreline.x_avg)&(df_shoreline.y_new<df_shoreline.y_avg),'shoreline_direction']=\\\n",
    "        [90+180*math.atan(np.abs(y_new-y_avg)/np.abs(x_new-x_avg))/np.pi for y_new,y_avg,x_new,x_avg in zip(\n",
    "                df_shoreline.y_new,\n",
    "                df_shoreline.y_avg,\n",
    "                df_shoreline.x_new,\n",
    "                df_shoreline.x_avg\n",
    "                ) if (x_new>x_avg)&(y_new<y_avg)]\n",
    "\n",
    "    # Quadrant 3\n",
    "    df_shoreline.loc[(df_shoreline.x_new<df_shoreline.x_avg)&(df_shoreline.y_new<df_shoreline.y_avg),'shoreline_direction']=\\\n",
    "        [180+180*math.atan(np.abs(x_new-x_avg)/np.abs(y_new-y_avg))/np.pi for y_new,y_avg,x_new,x_avg in zip(\n",
    "                df_shoreline.y_new,\n",
    "                df_shoreline.y_avg,\n",
    "                df_shoreline.x_new,\n",
    "                df_shoreline.x_avg\n",
    "                ) if (x_new<x_avg)&(y_new<y_avg)]\n",
    "\n",
    "    # Quadrant 4\n",
    "    df_shoreline.loc[(df_shoreline.x_new<df_shoreline.x_avg)&(df_shoreline.y_new>df_shoreline.y_avg),'shoreline_direction']=\\\n",
    "        [270+180*math.atan(np.abs(y_new-y_avg)/np.abs(x_new-x_avg))/np.pi for y_new,y_avg,x_new,x_avg in zip(\n",
    "                df_shoreline.y_new,\n",
    "                df_shoreline.y_avg,\n",
    "                df_shoreline.x_new,\n",
    "                df_shoreline.x_avg\n",
    "                ) if (x_new<x_avg)&(y_new>y_avg)]\n",
    "\n",
    "    return(df_shoreline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62328b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Nanumea', 'WM', 1, 2002.375, 2003.0416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2013.9583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2014.0416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2014.2083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2014.2916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2014.375)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2014.7083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2014.875)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2014.9583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2015.125)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2015.2916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2015.5416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2015.625)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2015.7916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2015.9583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2016.0416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2016.375)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2016.7916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2016.875)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2017.0416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2017.2916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2017.5416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2017.625)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2018.2083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2018.375)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2018.4583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2018.7083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2019.125)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2019.2916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2019.5416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2019.625)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2019.7916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2019.875)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.0416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.125)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.2083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.2916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.4583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.5416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.625)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.7083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.7916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.875)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2020.9583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.0416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.2083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.375)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.4583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.5416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.625)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.7083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.7916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.875)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2021.9583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.0416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.125)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.2083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.2916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.375)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.4583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.5416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.625)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.7083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.7916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.875)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2022.9583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.0416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.125)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.2083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.2916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.375)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.4583333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.5416666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.625)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.7083333333333)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.7916666666667)\n",
      "('Nanumea', 'WM', 1, 2002.375, 2023.9583333333333)\n"
     ]
    }
   ],
   "source": [
    "# For each year, proxy and islet combination, find the shoreline change\n",
    "shorelines_dict = {}\n",
    "\n",
    "# Each combination of atoll and proxy combination\n",
    "for key,item in df_xy_dict.items():\n",
    "    df_xy = item['df_xy']\n",
    "    gdf_xy = item['gdf_test']\n",
    "    \n",
    "    # Now get all the ids for this atoll\n",
    "    ids = np.unique(df_xy.id)\n",
    "    \n",
    "    for ID in ids:\n",
    "        df_xy_islet = df_xy[df_xy.id==ID]\n",
    "        gdf_xy_islet = gdf_xy[gdf_xy.id==ID]\n",
    "            \n",
    "        # Get all the years\n",
    "        years = np.sort(np.unique(df_xy_islet.year))\n",
    "        years_beginning = years[:-1]\n",
    "        years_end = years[1:]\n",
    "        \n",
    "        # Loop over the years, defining the beginning and the end year\n",
    "        for year_end in years[1:]:\n",
    "            \n",
    "            df_xy_islet_beginning = df_xy_islet[(df_xy_islet.year==years[0])]\n",
    "            df_xy_islet_end = df_xy_islet[(df_xy_islet.year==year_end)]\n",
    "            \n",
    "            df_shoreline_1 = create_shoreline_df(df_xy_islet_beginning).reset_index(drop=True)\n",
    "            df_shoreline_2 = create_shoreline_df(df_xy_islet_end).reset_index(drop=True)\n",
    "\n",
    "            df_shoreline_2 = calc_shoreline_change(df_shoreline_1,df_shoreline_2)\n",
    "\n",
    "            df_shoreline_2 = calc_shoreline_slope_change(df_shoreline_2)\n",
    "            \n",
    "            # Adding in the final year of the shoreline change dictionary\n",
    "            df_shoreline_2['year1'] = years[0]\n",
    "            df_shoreline_2['year2'] = year_end\n",
    "            \n",
    "            # Filter out the anomalies of anything greater than 100 m change\n",
    "            df_shoreline_2 = df_shoreline_2[np.abs(df_shoreline_2.intersect_distance)<100]\n",
    "            \n",
    "            shorelines_dict.update({\n",
    "                (key[0],key[1],ID,years[0],year_end):df_shoreline_2.to_dict()\n",
    "            })\n",
    "            print((key[0],key[1],ID,years[0],year_end))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab215baa",
   "metadata": {},
   "source": [
    "# Saved the Processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94f7f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Correct the wrong dates in the TOB, year 2008 should be 2007\n",
    "# for islet_id in [1,2,3]:\n",
    "#     df = shorelines_dict[('Nanumea', 'TOB', islet_id, 2003, 2008)].copy()\n",
    "#     df = pd.DataFrame.from_dict(df,orient='index').T\n",
    "#     df['year2'] = [2007]*len(df)\n",
    "#     del shorelines_dict[('Nanumea', 'TOB', islet_id, 2003, 2008)]\n",
    "#     shorelines_dict[('Nanumea', 'TOB', islet_id, 2003, 2007)] = df.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ffcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Processed_data_and_output/shorelines_dict.json','wb') as fp:\n",
    "    pickle.dump(shorelines_dict,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df7781-72bd-4977-836b-c7a25ddd8f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "339.984px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
