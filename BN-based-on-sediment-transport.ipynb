{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the most up-to-date version of the pilot model for total water level on Tarawa, currently for only two locations (one lagoon side and one ocean side).\n",
    "\n",
    "Currently needs work:\n",
    "- Incorporating MEI into network\n",
    "- Adjusting the binning of the MSL distributions to account for future SLR\n",
    "- Adding SLR projections as evidence option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:06.245034Z",
     "start_time": "2021-06-10T02:18:06.200060Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.215577Z",
     "start_time": "2021-06-10T02:18:06.246033Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pysmile\n",
    "import pysmile_license\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('/src/python_classes')\n",
    "import rpy2\n",
    "# os.environ['R_HOME'] = 'C:\\ProgramData\\Anaconda3\\Lib\\R'\n",
    "# %load_ext rpy2.ipython\n",
    "!jupyter nbextension enable --py --sys-prefix ipyleaflet\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipyleaflet import *\n",
    "import ipywidgets as widgets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import geojson\n",
    "import folium\n",
    "from colormap import rgb2hex\n",
    "import rpy2\n",
    "os.environ['R_HOME'] = '/lib/R'\n",
    "%load_ext rpy2.ipython\n",
    "from folium.plugins import FloatImage\n",
    "from shapely import geometry\n",
    "\n",
    "from BNModel import BNModel\n",
    "\n",
    "from preprocessing_all_points import *\n",
    "from preprocessing_points_spatially import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:18:09.301509Z",
     "start_time": "2021-06-10T02:18:09.217559Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### set location of file storage\n",
    "# folder = 'BN_antonio_data'\n",
    "# try:\n",
    "#     os.makedirs(folder)\n",
    "# except FileExistsError:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shoreline_change = pd.read_csv('wave_lag_vars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the NAN with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shoreline_change.replace(np.nan,0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shoreline_change.columns = \\\n",
    "    ['lon',\n",
    "     'lat',\n",
    "     'proxy',\n",
    "     'islet_id',\n",
    "     'atoll',\n",
    "     'beginning_year',\n",
    "     'end_year',\n",
    "     'intersect_distance',\n",
    "     'x_avg',\n",
    "     'y_avg',\n",
    "     'm_transect',\n",
    "     'c_transect',\n",
    "     'shoreline_direction',\n",
    "     'avg_slope_change',\n",
    "     \"NE_low_freq\",\n",
    "     \"NE_high_freq\",\n",
    "     \"ES_low_freq\",\n",
    "     \"ES_high_freq\",\n",
    "     \"SW_low_freq\",\n",
    "     \"SW_high_freq\",\n",
    "     \"WN_low_freq\",\n",
    "     \"WN_high_freq\",\n",
    "     'lag_intersect_distance',\n",
    "     'lag_intersect_distance_surroundings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shoreline_change.loc[df_shoreline_change.avg_slope_change==-0,'avg_slope_change']=0\n",
    "df_shoreline_change['avg_slope_change'] = ['angle'+str(x).replace('-','negative').replace('.0','') for x in df_shoreline_change.avg_slope_change]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shoreline_change_original = df_shoreline_change.copy()\n",
    "\n",
    "df_shoreline_change_reduced = df_shoreline_change[\\\n",
    "      (np.abs(df_shoreline_change.intersect_distance)<0.03) # np.abs(df_shoreline_change.intersect_distance)>=0.001)&\n",
    "                                                 ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shoreline_change_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_shoreline_change_reduced.intersect_distance,bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shoreline_change = df_shoreline_change_reduced.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shoreline_change.drop(['lon','lat'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Angle of Incidence Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_shoreline_change.lab_intersect_distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Variable Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.324519Z",
     "start_time": "2021-06-10T02:19:00.237568Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "shoreline_change_model_dict = {\n",
    "   'variables':{\n",
    "       'proxy':{\n",
    "            'label':'Shoreline Proxy',\n",
    "            'child_nodes':['intersect_distance'],\n",
    "            'bins':list(np.unique(df_shoreline_change.proxy))\n",
    "        },\n",
    "        'shoreline_direction':{\n",
    "            'label':'Direction of shoreline (degrees)',\n",
    "            'discretisation':{\n",
    "                'n_bins':4,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['NE','SE','SW','NW'] # I think these labels are wrong... because we start at -180\n",
    "            },\n",
    "            'child_nodes':['intersect_distance']\n",
    "        },\n",
    "        'avg_slope_change':{\n",
    "            'label':'Angle between adjacent shoreline segments (degrees)',\n",
    "            'child_nodes':['intersect_distance'],\n",
    "            'bins':list(np.unique(df_shoreline_change.avg_slope_change))\n",
    "        },\n",
    "        'intersect_distance':{\n",
    "            'label':'Distance shoreline has moved',\n",
    "            'discretisation':{\n",
    "                'n_bins':5,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "            },\n",
    "            'child_nodes':[]\n",
    "        },\n",
    "#         \"NE_low_freq\":{\n",
    "#             'label':\"(0.033, 0.1997, 'NE', 'mean')\",\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':3,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['Low','Mid','High']\n",
    "#             },\n",
    "#             'child_nodes':['intersect_distance']\n",
    "#         },\n",
    "        \"NE_high_freq\":{\n",
    "            'label':\"(0.1997, 1.0, 'NE', 'mean')\",\n",
    "            'discretisation':{\n",
    "                'n_bins':3,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['Low','Mid','High']\n",
    "            },\n",
    "            'child_nodes':['intersect_distance']\n",
    "        },\n",
    "#         \"ES_low_freq\":{\n",
    "#             'label':\"(0.033, 0.1997, 'ES', 'mean')\",\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':3,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['Low','Mid','High']\n",
    "#             },\n",
    "#             'child_nodes':['intersect_distance']\n",
    "#         },\n",
    "        \"ES_high_freq\":{\n",
    "            'label':\"(0.1997, 1.0, 'ES', 'mean')\",\n",
    "            'discretisation':{\n",
    "                'n_bins':3,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['Low','Mid','High']\n",
    "            },\n",
    "            'child_nodes':['intersect_distance']\n",
    "        },\n",
    "#         \"SW_low_freq\":{\n",
    "#             'label':\"(0.033, 0.1997, 'SW', 'mean')\",\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':3,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['Low','Mid','High']\n",
    "#             },\n",
    "#             'child_nodes':['intersect_distance']\n",
    "#         },\n",
    "        \"SW_high_freq\":{\n",
    "            'label':\"(0.1997, 1.0, 'SW', 'mean')\",\n",
    "            'discretisation':{\n",
    "                'n_bins':3,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['Low','Mid','High']\n",
    "            },\n",
    "            'child_nodes':['intersect_distance']\n",
    "        },\n",
    "#         \"WN_low_freq\":{\n",
    "#             'label':\"(0.033, 0.1997, 'WN', 'mean')\",\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':3,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['Low','Mid','High']\n",
    "#             },\n",
    "#             'child_nodes':['intersect_distance']\n",
    "#         },\n",
    "        \"WN_high_freq\":{\n",
    "            'label':\"(0.1997, 1.0, 'WN', 'mean')\",\n",
    "            'discretisation':{\n",
    "                'n_bins':3,\n",
    "                'strategy':'kmeans',\n",
    "                'bin_names':['Low','Mid','High']\n",
    "            },\n",
    "            'child_nodes':['intersect_distance']\n",
    "        }\n",
    "#         'Tide':{\n",
    "#             'label':'Tide',\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':5,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "#             },\n",
    "#             'child_nodes':['TWL']\n",
    "#         },\n",
    "#         'TWL':{\n",
    "#             'label':'Total water level',\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':7,\n",
    "#                 'strategy':'binned',\n",
    "#                 'bin_names':['VeryLow','Low','LowMid','Mid','MidHigh','High','VeryHigh'],\n",
    "#                 'bin_edges':np.arange(-1,3.0,0.5)\n",
    "#             },\n",
    "#             'child_nodes':[]\n",
    "#         },\n",
    "#         'TWL_less_Tide':{\n",
    "#             'label':'Total water level less tide',\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':5,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "#             },\n",
    "#             'child_nodes':['TWL']\n",
    "#         },\n",
    "#         'reef_width':{\n",
    "#             'label':'Reef width',\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':5,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "#             },\n",
    "#             'child_nodes':['TWL_less_Tide']\n",
    "#         },\n",
    "#         'reef_depth':{\n",
    "#             'label':'Reef depth',\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':5,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "#             },\n",
    "#             'child_nodes':['TWL_less_Tide']\n",
    "#         },\n",
    "#         'forereef_slope':{\n",
    "#             'label':'Fore reef slope',\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':5,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['VeryLow','Low','Mid','High','VeryHigh']\n",
    "#             },\n",
    "#             'child_nodes':['TWL_less_Tide']\n",
    "#         },\n",
    "#         'shore_dir':{\n",
    "#             'label':'Shoreline direction',\n",
    "#             'discretisation':{\n",
    "#                 'n_bins':3,\n",
    "#                 'strategy':'kmeans',\n",
    "#                 'bin_names':['NE','S','NW']\n",
    "#             },\n",
    "#             'child_nodes':['TWL_less_Tide']\n",
    "#         } \n",
    "   },\n",
    "    'training_frac':0.8,\n",
    "    'bootstrap_reps':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the data for the variables in the model dict\n",
    "df_temp = df_shoreline_change[list(shoreline_change_model_dict['variables'].keys())]\n",
    "shoreline_change_data_dict = {k:np.array(v) for k,v in df_temp.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.unique(df_shoreline_change.avg_slope_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoreline_change_data_dict['avg_slope_change']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:00.754301Z",
     "start_time": "2021-06-10T02:19:00.326513Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bootstrap the data, and add it to the model_dict\n",
    "shoreline_change_model_dict = BNModel().bootstrap_data(shoreline_change_model_dict,shoreline_change_data_dict,df_shoreline_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:46.021235Z",
     "start_time": "2021-06-10T02:19:00.755300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discretise the data\n",
    "shoreline_change_file_label = \"ocean\"\n",
    "\n",
    "shoreline_change_model_dict = BNModel().discretiser(shoreline_change_model_dict,[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the discretised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame.from_dict(\\\n",
    "   {key:var_dict['training_data'][0] for key,var_dict in shoreline_change_model_dict['variables'].items()}\n",
    "                              )\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.711518Z",
     "start_time": "2021-06-10T02:20:53.985216Z"
    }
   },
   "outputs": [],
   "source": [
    "BNModel().save_dataset(shoreline_change_model_dict,shoreline_change_file_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:21:05.864450Z",
     "start_time": "2021-06-10T02:21:05.152790Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shoreline_change_model_dict = BNModel().create_SM(shoreline_change_model_dict,shoreline_change_file_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at bins of intersection_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Conditional Probability tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.814431Z",
     "start_time": "2021-06-10T02:20:56.725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get conditional probs tables\n",
    "df_CPT_MSL = BNModel().get_conditional_prob_table(shoreline_change_model_dict,'intersect_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.822424Z",
     "start_time": "2021-06-10T02:20:56.885Z"
    }
   },
   "outputs": [],
   "source": [
    "df_CPT_MSL = df_CPT_MSL.loc[['VeryLow','Low','Mid','High','VeryHigh']]\n",
    "\n",
    "plt.pcolor(df_CPT_MSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.827422Z",
     "start_time": "2021-06-10T02:20:57.062Z"
    }
   },
   "outputs": [],
   "source": [
    "df_CPT_MSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_CPT_MSL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.833418Z",
     "start_time": "2021-06-10T02:20:57.381Z"
    }
   },
   "outputs": [],
   "source": [
    "shoreline_change_evidence_dict = {\n",
    "    'proxy':[1,0,0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.838415Z",
     "start_time": "2021-06-10T02:20:57.549Z"
    }
   },
   "outputs": [],
   "source": [
    "shoreline_change_model_dict = BNModel().add_evidence_to_dict(shoreline_change_model_dict,shoreline_change_evidence_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update beliefs based on evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:58.843410Z",
     "start_time": "2021-06-10T02:20:57.946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set evidence and get beliefs\n",
    "shoreline_change_model_dict = BNModel().update_evidence(shoreline_change_model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BN Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:20:59.261392Z",
     "start_time": "2021-06-10T02:20:59.150428Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set up graph\n",
    "graph_shoreline_change = BNModel().create_BN_graph()\n",
    "\n",
    "# Create nodes of the graph\n",
    "graph_shoreline_change,shoreline_change_model_dict = BNModel().create_nodes(graph_shoreline_change,shoreline_change_model_dict,0)\n",
    "\n",
    "# Create arcs between nodesb\n",
    "graph_shoreline_change = BNModel().create_arcs(graph_shoreline_change,shoreline_change_model_dict)\n",
    "\n",
    "# Save as dot file\n",
    "graph_shoreline_change.render(filename='graph_ocean',format='png')\n",
    "\n",
    "# Plot the graph\n",
    "graph_shoreline_change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T02:19:49.420270Z",
     "start_time": "2021-06-10T02:18:06.287Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "shoreline_change_acc_dict = BNModel().get_accuracies(shoreline_change_model_dict,\"intersect_distance\")\n",
    "\n",
    "print(shoreline_change_acc_dict)\n",
    "\n",
    "fig, ax = BNModel().confusion_matrix(shoreline_change_model_dict,\"intersect_distance\",0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise the Change in shoreline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,group in df_visualisation.groupby(['beginning_year','end_year']):\n",
    "#     print(group)\n",
    "#     asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_locator(value,bin_edges):\n",
    "    '''\n",
    "    function used for determining the index of the appropriate bin for a numerical value.\n",
    "    '''\n",
    "    i=0\n",
    "    for edge_1,edge_2 in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "        if (value>edge_1)&(value<=edge_2):\n",
    "            loc_bin = i\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    if value<=bin_edges[0]:\n",
    "        loc_bin = 0\n",
    "\n",
    "    if value>=bin_edges[-1]:\n",
    "        loc_bin = len(bin_edges)-2\n",
    "\n",
    "    return(loc_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atoll = 'Nanumea'\n",
    "proxy = 'WM'\n",
    "\n",
    "# Get subset of dataframe\n",
    "df_visualisation = df_shoreline_change_original.copy()\n",
    "df_visualisation = df_visualisation[\n",
    "    (df_visualisation.proxy==proxy)&\\\n",
    "    (df_visualisation.atoll==atoll)&\\\n",
    "    (df_visualisation.islet_id==1)]\n",
    "\n",
    "# Get dictionary of just bins\n",
    "bin_pt1 = [key for key,item in shoreline_change_model_dict['variables'].items() if 'bins' in item]\n",
    "bin_pt2 = [key for key,item in shoreline_change_model_dict['variables'].items() if 'discretisation' in item]\n",
    "bin_pt2.remove('intersect_distance') # remove intersect distance because this is what we are solving for \n",
    "\n",
    "for i in df_visualisation.index:\n",
    "    evidence_to_set = {key:df_visualisation.loc[i,key] for key,item in shoreline_change_model_dict['variables'].items()}\n",
    "\n",
    "    evidence_dict = {}\n",
    "\n",
    "    for key in bin_pt2:\n",
    "        bin_edges = shoreline_change_model_dict['variables'][key]['bin_edges'][0]\n",
    "        evidence = evidence_to_set[key]\n",
    "        var_bin = bin_locator(evidence,bin_edges)\n",
    "        evidence_array = [0]*(len(bin_edges)-1)\n",
    "        evidence_array[var_bin] = 1\n",
    "\n",
    "        evidence_dict.update({\n",
    "            key:evidence_array\n",
    "        })\n",
    "\n",
    "    for key in bin_pt1:\n",
    "        evidence = evidence_to_set[key]\n",
    "        if evidence==-0:\n",
    "            evidence = float(0)\n",
    "        evidence = str(evidence)\n",
    "        bins = shoreline_change_model_dict['variables'][key]['bins']\n",
    "        evidence_array = [1 if evidence==x else 0 for x in bins]\n",
    "\n",
    "        evidence_dict.update({\n",
    "            key:evidence_array\n",
    "        })\n",
    "\n",
    "    # Add evidence to model dict\n",
    "    model_location_dict = BNModel().add_evidence_to_dict(shoreline_change_model_dict,evidence_dict)\n",
    "\n",
    "    # Set evidence and get beliefs\n",
    "    model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "\n",
    "    # get the resulting probabilities\n",
    "    resulting_probs = model_location_dict['variables']['intersect_distance']['resulting_probs'][0]\n",
    "\n",
    "    # get bin information, and calculate the most likely change in distance of the shoreline according to the BN\n",
    "    bin_edges = model_location_dict['variables']['intersect_distance']['bin_edges'][0]\n",
    "    training_data = model_location_dict['variables']['intersect_distance']['training_data_preprocessed'][0] # Get the training data\n",
    "    bin_means = [] # For each bin, get the mean of the training data\n",
    "    for bin_min,bin_max in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "        bin_means.append(np.mean(training_data[(training_data<bin_max)&(training_data>bin_min)]))    \n",
    "    df_probs = pd.DataFrame.from_dict(resulting_probs,orient='index')\n",
    "    df_probs.columns = ['probs']\n",
    "    df_probs['bin_means'] = bin_means\n",
    "    max_probs_int_dist = df_probs.loc[(df_probs.probs==np.max(df_probs.probs)),'bin_means']\n",
    "    max_prob_int_dist = np.mean(max_probs_int_dist)\n",
    "\n",
    "    df_visualisation.loc[i,'shoreline_movement'] = max_prob_int_dist\n",
    "\n",
    "\n",
    "# Get constants for quadratic equation\n",
    "a = df_visualisation.m_transect**2+1\n",
    "b = -2*df_visualisation.y_avg*df_visualisation.m_transect+2*df_visualisation.c_transect*df_visualisation.m_transect-2*df_visualisation.x_avg\n",
    "c = df_visualisation.x_avg**2+df_visualisation.y_avg**2+df_visualisation.c_transect**2-2*df_visualisation.y_avg*df_visualisation.c_transect-df_visualisation.shoreline_movement**2\n",
    "\n",
    "# Filter out the locations where it fails for some reason\n",
    "t = b**2-4*a*c\n",
    "df_visualisation = df_visualisation[df_visualisation.index.isin(t[t>0].index)]\n",
    "\n",
    "# Recalc quadratic equation constants\n",
    "a = df_visualisation.m_transect**2+1\n",
    "b = -2*df_visualisation.y_avg*df_visualisation.m_transect+2*df_visualisation.c_transect*df_visualisation.m_transect-2*df_visualisation.x_avg\n",
    "c = df_visualisation.x_avg**2+df_visualisation.y_avg**2+df_visualisation.c_transect**2-2*df_visualisation.y_avg*df_visualisation.c_transect-df_visualisation.shoreline_movement**2\n",
    "\n",
    "# get the two estimates for x, and pick based on quadrant\n",
    "df_visualisation['x_est1'] = [(-bi+math.sqrt(bi**2-4*ai*ci))/(2*ai) for ai,bi,ci in zip(a,b,c)]\n",
    "df_visualisation['x_est2'] = [(-bi-math.sqrt(bi**2-4*ai*ci))/(2*ai) for ai,bi,ci in zip(a,b,c)] \n",
    "df_visualisation.loc[(df_visualisation.shoreline_direction<=180),'x_est'] = df_visualisation.loc[(df_visualisation.shoreline_direction<=180),'x_est1']\n",
    "df_visualisation.loc[(df_visualisation.shoreline_direction>180),'x_est'] = df_visualisation.loc[(df_visualisation.shoreline_direction>180),'x_est2']\n",
    "df_visualisation['y_est'] = df_visualisation.m_transect*df_visualisation.x_est+df_visualisation.c_transect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning_year = 2010\n",
    "df_visualisation = df_shoreline_change_original.copy()\n",
    "df_visualisation = df_visualisation[\n",
    "    (df_visualisation.proxy==proxy)&\\\n",
    "    (df_visualisation.atoll==atoll)&\\\n",
    "    (df_visualisation.beginning_year==beginning_year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vis_df(atoll,proxy,beginning_year):\n",
    "    # Get subset of dataframe\n",
    "    df_visualisation = df_shoreline_change_original.copy()\n",
    "    df_visualisation = df_visualisation[\n",
    "        (df_visualisation.proxy==proxy)&\\\n",
    "        (df_visualisation.atoll==atoll)&\\\n",
    "        (df_visualisation.beginning_year==beginning_year)]\n",
    "    \n",
    "    # get the end year to use to select the next polygon\n",
    "    end_year = np.unique(df_visualisation.end_year)[0]\n",
    "\n",
    "    # Get dictionary of just bins\n",
    "    bin_pt1 = [key for key,item in shoreline_change_model_dict['variables'].items() if 'bins' in item]\n",
    "    bin_pt2 = [key for key,item in shoreline_change_model_dict['variables'].items() if 'discretisation' in item]\n",
    "    bin_pt2.remove('intersect_distance') # remove intersect distance because this is what we are solving for \n",
    "\n",
    "    for i in df_visualisation.index:\n",
    "        evidence_to_set = {key:df_visualisation.loc[i,key] for key,item in shoreline_change_model_dict['variables'].items()}\n",
    "\n",
    "        evidence_dict = {}\n",
    "\n",
    "        for key in bin_pt2:\n",
    "            bin_edges = shoreline_change_model_dict['variables'][key]['bin_edges'][0]\n",
    "            evidence = evidence_to_set[key]\n",
    "            var_bin = bin_locator(evidence,bin_edges)\n",
    "            evidence_array = [0]*(len(bin_edges)-1)\n",
    "            evidence_array[var_bin] = 1\n",
    "\n",
    "            evidence_dict.update({\n",
    "                key:evidence_array\n",
    "            })\n",
    "\n",
    "        for key in bin_pt1:\n",
    "            evidence = evidence_to_set[key]\n",
    "            if evidence==-0:\n",
    "                evidence = float(0)\n",
    "            evidence = str(evidence)\n",
    "            bins = shoreline_change_model_dict['variables'][key]['bins']\n",
    "            evidence_array = [1 if evidence==x else 0 for x in bins]\n",
    "\n",
    "            evidence_dict.update({\n",
    "                key:evidence_array\n",
    "            })\n",
    "\n",
    "        # Add evidence to model dict\n",
    "        model_location_dict = BNModel().add_evidence_to_dict(shoreline_change_model_dict,evidence_dict)\n",
    "\n",
    "        # Set evidence and get beliefs\n",
    "        model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "\n",
    "        # get the resulting probabilities\n",
    "        resulting_probs = model_location_dict['variables']['intersect_distance']['resulting_probs'][0]\n",
    "\n",
    "        # get bin information, and calculate the most likely change in distance of the shoreline according to the BN\n",
    "        bin_edges = model_location_dict['variables']['intersect_distance']['bin_edges'][0]\n",
    "        training_data = model_location_dict['variables']['intersect_distance']['training_data_preprocessed'][0] # Get the training data\n",
    "        bin_means = [] # For each bin, get the mean of the training data\n",
    "        for bin_min,bin_max in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "            bin_means.append(np.mean(training_data[(training_data<bin_max)&(training_data>bin_min)]))    \n",
    "        df_probs = pd.DataFrame.from_dict(resulting_probs,orient='index')\n",
    "        df_probs.columns = ['probs']\n",
    "        df_probs['bin_means'] = bin_means\n",
    "        max_probs_int_dist = df_probs.loc[(df_probs.probs==np.max(df_probs.probs)),'bin_means']\n",
    "        max_prob_int_dist = np.mean(max_probs_int_dist)\n",
    "\n",
    "        df_visualisation.loc[i,'shoreline_movement'] = max_prob_int_dist\n",
    "\n",
    "\n",
    "    # Get constants for quadratic equation\n",
    "    a = df_visualisation.m_transect**2+1\n",
    "    b = -2*df_visualisation.y_avg*df_visualisation.m_transect+2*df_visualisation.c_transect*df_visualisation.m_transect-2*df_visualisation.x_avg\n",
    "    c = df_visualisation.x_avg**2+df_visualisation.y_avg**2+df_visualisation.c_transect**2-2*df_visualisation.y_avg*df_visualisation.c_transect-df_visualisation.shoreline_movement**2\n",
    "\n",
    "    # Filter out the locations where it fails for some reason\n",
    "    t = b**2-4*a*c\n",
    "    df_visualisation = df_visualisation[df_visualisation.index.isin(t[t>0].index)]\n",
    "\n",
    "    # Recalc quadratic equation constants\n",
    "    a = df_visualisation.m_transect**2+1\n",
    "    b = -2*df_visualisation.y_avg*df_visualisation.m_transect+2*df_visualisation.c_transect*df_visualisation.m_transect-2*df_visualisation.x_avg\n",
    "    c = df_visualisation.x_avg**2+df_visualisation.y_avg**2+df_visualisation.c_transect**2-2*df_visualisation.y_avg*df_visualisation.c_transect-df_visualisation.shoreline_movement**2\n",
    "    \n",
    "    # get the two estimates for x, and pick based on quadrant\n",
    "    df_visualisation['x_est1'] = [(-bi+math.sqrt(bi**2-4*ai*ci))/(2*ai) for ai,bi,ci in zip(a,b,c)]\n",
    "    df_visualisation['x_est2'] = [(-bi-math.sqrt(bi**2-4*ai*ci))/(2*ai) for ai,bi,ci in zip(a,b,c)] \n",
    "    df_visualisation.loc[(df_visualisation.shoreline_direction<=180),'x_est'] = df_visualisation.loc[(df_visualisation.shoreline_direction<=180),'x_est1']\n",
    "    df_visualisation.loc[(df_visualisation.shoreline_direction>180),'x_est'] = df_visualisation.loc[(df_visualisation.shoreline_direction>180),'x_est2']\n",
    "    df_visualisation['y_est'] = df_visualisation.m_transect*df_visualisation.x_est+df_visualisation.c_transect\n",
    "    \n",
    "    return(df_visualisation,end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_figure(view,atoll,proxy,beginning_year):\n",
    "    \n",
    "    if atoll=='Nanumaga':\n",
    "        coords = [-6.286804, 176.320524]\n",
    "    elif atoll=='Nanumea':\n",
    "        coords = [-5.664393, 176.108576]\n",
    "    else:\n",
    "        print('No coords for atoll')\n",
    "    \n",
    "    # Nanumanga -6.286804, 176.320524; nanumea -5.664393, 176.108576\n",
    "    \n",
    "    if view == 'Map':\n",
    "        map_osm = folium.Map(location=coords,zoom_start=14)\n",
    "    elif view == 'Satellite':\n",
    "        token = \"pk.eyJ1Ijoic2hhbm5vbi1iZW5ndHNvbiIsImEiOiJja3F1Y2Q0dHEwMzYwMm9wYmtzYzk2bDZuIn0.5jGMyEiJdmXs1HL7x3ThPw\" # your mapbox token\n",
    "        tileurl = 'https://api.mapbox.com/v4/mapbox.satellite/{z}/{x}/{y}@2x.png?access_token=' + str(token)\n",
    "\n",
    "        map_osm = folium.Map(location=coords, zoom_start=14, tiles=tileurl, attr='Mapbox')\n",
    "        \n",
    "    intersect_distance_bin_edges = [round(x,2) for x in shoreline_change_model_dict['variables']['intersect_distance']['bin_edges'][0]]\n",
    "    intersect_distance_bins = shoreline_change_model_dict['variables']['intersect_distance']['discretisation']['bin_names']\n",
    "\n",
    "    colours_rgb = matplotlib.pyplot.get_cmap('seismic')(np.arange(0,1+1/len(intersect_distance_bins),1/(len(intersect_distance_bins)-1)))\n",
    "    colour_hex_dict = {i:rgb2hex(int(255*colours_rgb[i][0]),int(255*colours_rgb[i][1]),int(255*(colours_rgb[i][2]))) for i in np.arange(0,len(intersect_distance_bins),1)}\n",
    "\n",
    "    #####################################################################\n",
    "    # Create visualisation dataframe for this atoll and proxy\n",
    "    df_visualisation,end_year = create_vis_df(atoll,proxy,beginning_year)\n",
    "    df_visualisation_actual,_ = create_vis_df(atoll,proxy,end_year)\n",
    "    \n",
    "    #####################################################################\n",
    "    # Add the original, estimated change, and actual shoreline change\n",
    "    \n",
    "    for index,group in df_visualisation.groupby('islet_id'):\n",
    "        poly = geometry.LineString([[g.lon,g.lat] for idx,g in group.iterrows()])\n",
    "        folium.Choropleth(\n",
    "                    poly,\n",
    "                    line_color=colour_hex_dict[1]\n",
    "                ).add_to(map_osm)\n",
    "    \n",
    "    #####################################################################\n",
    "    for index,group in df_visualisation.groupby('islet_id'):\n",
    "        poly_est = geometry.LineString([[g.x_est,g.y_est] for idx,g in group.iterrows()])\n",
    "        folium.Choropleth(\n",
    "                    poly_est,\n",
    "                    line_color=colour_hex_dict[4]\n",
    "                ).add_to(map_osm)\n",
    "    \n",
    "    for index,group in df_visualisation_actual.groupby('islet_id'):\n",
    "        poly_act = geometry.LineString([[g.lon,g.lat] for idx,g in group.iterrows()])\n",
    "        folium.Choropleth(\n",
    "                    poly_act,\n",
    "                    line_color=colour_hex_dict[3]\n",
    "                ).add_to(map_osm)\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "        \n",
    "    output_list = []\n",
    "    for rgb_color in colours_rgb:\n",
    "        output = plt.scatter([],[],color=rgb_color)\n",
    "        output_list.append(output)\n",
    "\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig('legend.png')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    url = (\n",
    "        \"legend.png\"\n",
    "    )    \n",
    "    \n",
    "    FloatImage(url, bottom=55, left=55).add_to(map_osm)\n",
    "    \n",
    "    map_osm.save('test.html')\n",
    "        \n",
    "    return(map_osm)\n",
    "    \n",
    "\n",
    "# compile the figure\n",
    "# lagoon_model_dict,shoreline_change_model_dict = initialise_model_dictionaries()\n",
    "proxy_bins = shoreline_change_model_dict['variables']['proxy']['bins']\n",
    "shoreline_direction_bins = shoreline_change_model_dict['variables']['shoreline_direction']['discretisation']['bin_names']\n",
    "avg_slope_change = shoreline_change_model_dict['variables']['avg_slope_change']['bins']\n",
    "NE_high_freq_bin = shoreline_change_model_dict['variables']['NE_high_freq']['discretisation']['bin_names']\n",
    "ES_high_freq_bin = shoreline_change_model_dict['variables']['NE_high_freq']['discretisation']['bin_names']\n",
    "SW_high_freq_bin = shoreline_change_model_dict['variables']['SW_high_freq']['discretisation']['bin_names']\n",
    "WN_high_freq_bin = shoreline_change_model_dict['variables']['WN_high_freq']['discretisation']['bin_names']\n",
    "\n",
    "# time = list(model_dicts_through_time_dict.keys())  \n",
    "\n",
    "\n",
    "# Create the plot with the widget\n",
    "map_osm = interact(test_figure,\n",
    "                view = widgets.Dropdown(options=['Map','Satellite'],value='Map',description='View type',disabled=False),\n",
    "                atoll = widgets.Dropdown(options=np.unique(df_shoreline_change.atoll),value='Nanumea',description='Atoll',disabled=False),\n",
    "                proxy = widgets.Dropdown(options=np.unique(df_shoreline_change.proxy),value='WM',description='Proxy',disabled=False),\n",
    "                beginning_year = widgets.Dropdown(options=np.unique(df_shoreline_change.beginning_year),value=2010,description='Beginning year',disabled=False),\n",
    "#                 proxy_bins = widgets.Dropdown(options=proxy_bins,value='TOB',description='Proxy',disabled=False),\n",
    "#                 shoreline_direction_bins = widgets.Dropdown(options=shoreline_direction_bins,description='Shoreline Direction',disabled=False),\n",
    "#                 avg_slope_change = widgets.Dropdown(options=avg_slope_change,description='Average slope change',disabled=False),\n",
    "#                 NE_high_freq_bin = widgets.Dropdown(options=NE_high_freq_bin,description='NE Wave Energy',disabled=False),\n",
    "#                 ES_high_freq_bin = widgets.Dropdown(options=ES_high_freq_bin,description='ES Wave Energy',disabled=False),\n",
    "#                 SW_high_freq_bin = widgets.Dropdown(options=SW_high_freq_bin,description='SW Wave Energy',disabled=False),\n",
    "#                 WN_high_freq_bin = widgets.Dropdown(options=WN_high_freq_bin,description='WN Wave Energy',disabled=False),   \n",
    "               )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoreline_change_model_dict['variables']['intersect_distance']['bin_edges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{key:[1 if str(evidence_to_set[key])==str(x) else 0 for x in bin_pt1[key]] if key in bin_pt1.keys() else 0\n",
    "     for key,item in evidence_to_set.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_to_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoreline_change_model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_locator(value,bin_edges):\n",
    "    '''\n",
    "    function used for determining the index of the appropriate bin for a numerical value.\n",
    "    '''\n",
    "    i=0\n",
    "    for edge_1,edge_2 in zip(bin_edges[:-1],bin_edges[1:]):\n",
    "        if (value>edge_1)&(value<=edge_2):\n",
    "            loc_bin = i\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    if value<=bin_edges[0]:\n",
    "        loc_bin = 0\n",
    "\n",
    "    if value>=bin_edges[-1]:\n",
    "        loc_bin = len(bin_edges)-2\n",
    "\n",
    "    return(loc_bin)\n",
    "\n",
    "def model_location(model_dict,location_details,evidence_dict,variable_list):\n",
    "    \n",
    "    '''\n",
    "    function for adding the location information for one side model to the evidence dictionary\n",
    "    '''\n",
    "    \n",
    "    for variable in variable_list:\n",
    "\n",
    "        bin_edges = model_dict['variables'][variable]['bin_edges'][0]\n",
    "        value = location_details[variable]\n",
    "\n",
    "        var_bin = bin_locator(value,bin_edges)\n",
    "        \n",
    "        evidence_array = [0]*(len(bin_edges)-1)\n",
    "        evidence_array[var_bin] = 1\n",
    "        \n",
    "        evidence_dict.update({\n",
    "            variable:evidence_array\n",
    "        })\n",
    "\n",
    "    # Add evidence to model dict\n",
    "    model_location_dict = BNModel().add_evidence_to_dict(model_dict,evidence_dict)\n",
    "    \n",
    "    # Set evidence and get beliefs\n",
    "    model_location_dict = BNModel().update_evidence(model_location_dict)\n",
    "    \n",
    "    return(model_location_dict)\n",
    "\n",
    "def location_probabilities(evidence_dict,model_dict,variable_list,df_profiles):\n",
    "    '''\n",
    "    \n",
    "    Function for setting evidence and determing probabilties for twl at each point around the island based \n",
    "    on the reef characteristics at each location\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    location_probabilities_dict = {}\n",
    "    \n",
    "    for index,row in df_profiles.iterrows():\n",
    "\n",
    "        model_location_dict = model_location(model_dict,row,evidence_dict,variable_list)\n",
    "        location_probabilities = model_location_dict['variables']['TWL']['resulting_probs'][0]\n",
    "\n",
    "        df_location_probabilities = pd.DataFrame.from_dict(location_probabilities,orient='index')\n",
    "        \n",
    "        largest_cat = df_location_probabilities.idxmax()[0]\n",
    "\n",
    "        location_probabilities_dict.update({\n",
    "            (row.reef_long,row.reef_lat):\\\n",
    "                model_dict['variables']['TWL']['discretisation']['bin_names'].index(largest_cat)\n",
    "        })\n",
    "        \n",
    "    return(location_probabilities_dict)\n",
    "\n",
    "def data2geojson(df):\n",
    "    features = []\n",
    "    insert_features = lambda X: features.append(\n",
    "            geojson.Feature(geometry=geojson.Point((X[\"long\"],\n",
    "                                                    X[\"lat\"])),\n",
    "                            properties=dict(name=X[\"most_likely_twl\"])))\n",
    "    df.apply(insert_features, axis=1)\n",
    "        \n",
    "    return(geojson.FeatureCollection(features))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_figure(\n",
    "    view#,proxy_bins,shoreline_direction_bins,avg_slope_change,NE_high_freq_bin,ES_high_freq_bin,SW_high_freq_bin,WN_high_freq_bin\n",
    "    ):\n",
    "        \n",
    "    if view == 'Map':\n",
    "        map_osm = folium.Map(location=[1.448888, 172.991794],zoom_start=11)\n",
    "    elif view == 'Satellite':\n",
    "        token = \"pk.eyJ1Ijoic2hhbm5vbi1iZW5ndHNvbiIsImEiOiJja3F1Y2Q0dHEwMzYwMm9wYmtzYzk2bDZuIn0.5jGMyEiJdmXs1HL7x3ThPw\" # your mapbox token\n",
    "        tileurl = 'https://api.mapbox.com/v4/mapbox.satellite/{z}/{x}/{y}@2x.png?access_token=' + str(token)\n",
    "\n",
    "        map_osm = folium.Map(location=[1.448888, 172.991794], zoom_start=11, tiles=tileurl, attr='Mapbox')\n",
    "        \n",
    "    intersect_distance_bin_edges = [round(x,2) for x in shoreline_change_model_dict['variables']['intersect_distance']['bin_edges'][0]]\n",
    "    intersect_distance_bins = shoreline_change_model_dict['variables']['intersect_distance']['discretisation']['bin_names']\n",
    "\n",
    "    colours_rgb = matplotlib.pyplot.get_cmap('seismic')(np.arange(0,1+1/len(intersect_distance_bins),1/(len(intersect_distance_bins)-1)))\n",
    "    colour_hex_dict = {i:rgb2hex(int(255*colours_rgb[i][0]),int(255*colours_rgb[i][1]),int(255*(colours_rgb[i][2]))) for i in np.arange(0,len(intersect_distance_bins),1)}\n",
    "    \n",
    "    ################################################\n",
    "    \n",
    "    # Create an empty dictionary for the evidence and populate as you go\n",
    "#     shoreline_evidence_dict = {}\n",
    "    \n",
    "#     for var_bin,var_name in zip([proxy_bins,shoreline_direction_bins,avg_slope_change,NE_high_freq_bin,ES_high_freq_bin,SW_high_freq_bin,WN_high_freq_bin],\n",
    "#                                 ['proxy','shoreline_direction','avg_slope_change','NE_high_freq','ES_high_freq','SW_high_freq','WN_high_freq']):\n",
    "\n",
    "#         ## Set in the evidence dict to be as indicated in the dropdown\n",
    "#         try:\n",
    "#             bin_index = shoreline_change_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "#             # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "#             evidence = [0 for x in shoreline_change_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "#         except:\n",
    "#             bin_index = shoreline_change_model_dict['variables'][var_name]['bins'].index(var_bin)\n",
    "#             # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "#             evidence = [0 for x in shoreline_change_model_dict['variables'][var_name]['bins']]\n",
    "            \n",
    "#         evidence[bin_index] = 1\n",
    "#         shoreline_evidence_dict.update({\n",
    "#             var_name:evidence\n",
    "#         })\n",
    "    \n",
    "    # Create a list of variables that are location specific to set as evidence in the network\n",
    "    variable_list = ['reef_width','reef_depth','forereef_slope','shore_dir']\n",
    "    \n",
    "    # get the probability dictionary\n",
    "    location_probabilities_dict = location_probabilities(shoreline_evidence_dict,shoreline_change_model_dict,variable_list,df_ocean_profiles)\n",
    "    \n",
    "    # Create dataframe to plot\n",
    "    df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "    df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    data_ocean = data2geojson(df_twl_locations)\n",
    "    \n",
    "    colors_hex_points_ocean = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    # Create an empty dictionary for the evidence and populate as you go\n",
    "    lagoon_evidence_dict = {}\n",
    "    \n",
    "    for var_bin,var_name in zip([tide_bin,msl_bin,wave_height_bin,wave_period_bin,wave_direction_bin,wind_u_bin,wind_v_bin],\n",
    "                                ['Tide','MSL','Hs_offshore','Tm_offshore','Dir_offshore','wind_u','wind_v']):\n",
    "\n",
    "        ## Set in the evidence dict to be as indicated in the dropdown\n",
    "        \n",
    "        bin_index = lagoon_model_dict['variables'][var_name]['discretisation']['bin_names'].index(var_bin)\n",
    "        # Create a list of the tide evidence (all zero except as indicated by dropdown. Dropdown=1)\n",
    "        evidence = [0 for x in lagoon_model_dict['variables'][var_name]['discretisation']['bin_names']]\n",
    "        evidence[bin_index] = 1\n",
    "        lagoon_evidence_dict.update({\n",
    "            var_name:evidence\n",
    "        })\n",
    "    \n",
    "    # Create a list of variables that are location specific to set as evidence in the network\n",
    "    variable_list = []\n",
    "    \n",
    "    # get the probability dictionary\n",
    "    location_probabilities_dict = location_probabilities(lagoon_evidence_dict,lagoon_model_dict,variable_list,df_lagoon_profiles)\n",
    "    \n",
    "    # Create dataframe to plot\n",
    "    df_twl_locations = pd.DataFrame.from_dict(location_probabilities_dict,orient='index').rename(columns={0:'most_likely_twl'})\n",
    "    df_twl_locations['long'] = [long for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations['lat'] = [lat for long,lat in df_twl_locations.index]\n",
    "    df_twl_locations.reset_index(drop=True,inplace=True)\n",
    "    shoreline_change_model_dict\n",
    "    data_lagoon = data2geojson(df_twl_locations)\n",
    "    \n",
    "    colors_hex_points_lagoon = [colour_hex_dict[x] for x in df_twl_locations.most_likely_twl]\n",
    "    \n",
    "    #####################################################################\n",
    "    \n",
    "    features_list = data_ocean['features']+data_lagoon['features']\n",
    "    \n",
    "    data = data_ocean\n",
    "    data.update({\n",
    "        'features':features_list\n",
    "    })\n",
    "    \n",
    "    colors_hex_points = colors_hex_points_ocean+colors_hex_points_lagoon\n",
    "    \n",
    "    #####################################################################\n",
    "\n",
    "    for feature,color in zip(features_list,colors_hex_points):\n",
    "        feature['properties'] = {'color':color,'weight':1,'markerColor':color,'fillOpacity':1,'fillColor':color}\n",
    "        long,lat = feature['geometry']['coordinates']\n",
    "        \n",
    "        marker = folium.CircleMarker([lat,long],color=color,\n",
    "                                    # popup='<img src={}_{}.png>'.format(int(long*1000),int(lat*1000)),\n",
    "                                   fill_color=color,fill=True,fill_opacity='1',radius=5)\n",
    "        marker.add_to(map_osm)\n",
    "        \n",
    "    twl_bin_edge_labels = ['{} to {} m'.format(\n",
    "        x,y) for x,y in zip(twl_bin_edges[:-1],twl_bin_edges[1:])]\n",
    "        \n",
    "    output_list = []\n",
    "    for rgb_color in colours_rgb:\n",
    "        output = plt.scatter([],[],color=rgb_color)\n",
    "        output_list.append(output)\n",
    "        \n",
    "    legend = plt.legend(output_list,twl_bin_edge_labels,title='Total water level anomaly',fontsize=10)\n",
    "    plt.setp(legend.get_title(),fontsize=12)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig('legend.png')\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    url = (\n",
    "        \"legend.png\"\n",
    "    )    \n",
    "    \n",
    "    FloatImage(url, bottom=55, left=55).add_to(map_osm)\n",
    "    \n",
    "    map_osm.save('test.html')\n",
    "        \n",
    "    return(map_osm)\n",
    "    \n",
    "\n",
    "# compile the figure\n",
    "# lagoon_model_dict,shoreline_change_model_dict = initialise_model_dictionaries()\n",
    "proxy_bins = shoreline_change_model_dict['variables']['proxy']['bins']\n",
    "shoreline_direction_bins = shoreline_change_model_dict['variables']['shoreline_direction']['discretisation']['bin_names']\n",
    "avg_slope_change = shoreline_change_model_dict['variables']['avg_slope_change']['bins']\n",
    "NE_high_freq_bin = shoreline_change_model_dict['variables']['NE_high_freq']['discretisation']['bin_names']\n",
    "ES_high_freq_bin = shoreline_change_model_dict['variables']['NE_high_freq']['discretisation']['bin_names']\n",
    "SW_high_freq_bin = shoreline_change_model_dict['variables']['SW_high_freq']['discretisation']['bin_names']\n",
    "WN_high_freq_bin = shoreline_change_model_dict['variables']['WN_high_freq']['discretisation']['bin_names']\n",
    "\n",
    "# time = list(model_dicts_through_time_dict.keys())  \n",
    "\n",
    "\n",
    "# Create the plot with the widget\n",
    "map_osm = interact(test_figure,\n",
    "                view = widgets.Dropdown(options=['Map','Satellite'],value='Map',description='View type',disabled=False),\n",
    "                proxy_bins = widgets.Dropdown(options=proxy_bins,value='TOB',description='Proxy',disabled=False),\n",
    "                shoreline_direction_bins = widgets.Dropdown(options=shoreline_direction_bins,description='Shoreline Direction',disabled=False),\n",
    "                avg_slope_change = widgets.Dropdown(options=avg_slope_change,description='Average slope change',disabled=False),\n",
    "                NE_high_freq_bin = widgets.Dropdown(options=NE_high_freq_bin,description='NE Wave Energy',disabled=False),\n",
    "                ES_high_freq_bin = widgets.Dropdown(options=ES_high_freq_bin,description='ES Wave Energy',disabled=False),\n",
    "                SW_high_freq_bin = widgets.Dropdown(options=SW_high_freq_bin,description='SW Wave Energy',disabled=False),\n",
    "                WN_high_freq_bin = widgets.Dropdown(options=WN_high_freq_bin,description='WN Wave Energy',disabled=False),\n",
    "                   \n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoreline_change_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "2344856/I2LE4LVY": {
     "DOI": "10.1029/2019PA003589",
     "author": [
      {
       "family": "Bengtson",
       "given": "Shannon A."
      },
      {
       "family": "Meissner",
       "given": "Katrin J."
      },
      {
       "family": "Menviel",
       "given": "Laurie"
      },
      {
       "family": "A. Sisson",
       "given": "Scott"
      },
      {
       "family": "Wilkin",
       "given": "John"
      }
     ],
     "container-title": "Paleoceanography and Paleoclimatology",
     "container-title-short": "Paleoceanography and Paleoclimatology",
     "id": "2344856/I2LE4LVY",
     "issued": {
      "day": 17,
      "month": 5,
      "year": 2019
     },
     "journalAbbreviation": "Paleoceanography and Paleoclimatology",
     "note": "Citation Key: bengtson2019evaluating",
     "page": "1022-1036",
     "page-first": "1022",
     "title": "Evaluating the extent of North Atlantic Deep Water and the mean Atlantic δ<sup>13</sup>C from statistical reconstructions",
     "type": "article-journal",
     "volume": "34"
    },
    "2344856/V5HIVSEQ": {
     "DOI": "10.1017/S0263593300020782",
     "URL": "https://www.cambridge.org/core/journals/earth-and-environmental-science-transactions-of-royal-society-of-edinburgh/article/an-alternative-astronomical-calibration-of-the-lower-pleistocene-timescale-based-on-odp-site-677/D02E93BFBF418256AD00642C8A98277C",
     "abstract": "Ocean Drilling Program (ODP) Site 677 provided excellent material for high resolution stable isotope analysis of both benthonic and planktonic foraminifera through the entire Pleistocene and upper Pliocene. The oxygen isotope record is readily correlated with the SPECMAP stack (Imbrie et al. 1984) and with the record from DSDP 607 (Ruddiman et al. 1986) but a significantly better match with orbital models is obtained by departing from the timescale proposed by these authors below Stage 16 (620 000 years). It is the stronger contribution from the precession signal in the record from ODP Site 677 that provides the basis for the revised timescale. Our proposed modification to the timescale would imply that the currently adopted radiometric dates for the Matuyama–Brunhes boundary, the Jaramillo and Olduvai Subchrons and the Gauss–Matuyama boundary underestimate their true astronomical ages by between 5 and 7%.",
     "accessed": {
      "day": 19,
      "month": 5,
      "year": 2020
     },
     "author": [
      {
       "family": "Shackleton",
       "given": "N. J."
      },
      {
       "family": "Berger",
       "given": "A."
      },
      {
       "family": "Peltier",
       "given": "W. R."
      }
     ],
     "container-title": "Earth and Environmental Science Transactions of The Royal Society of Edinburgh",
     "id": "2344856/V5HIVSEQ",
     "issue": "4",
     "issued": {
      "year": 1990
     },
     "language": "en",
     "note": "citation key: shackleton1990alternative",
     "page": "251-261",
     "page-first": "251",
     "title": "An alternative astronomical calibration of the lower Pleistocene timescale based on ODP Site 677",
     "type": "article-journal",
     "volume": "81"
    }
   }
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "744px",
    "left": "1262px",
    "right": "20px",
    "top": "135px",
    "width": "279px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
